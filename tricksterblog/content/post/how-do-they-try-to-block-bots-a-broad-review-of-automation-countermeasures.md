+++
author = "rl1987"
title = "How do they (try to) block bots: a broad review of automation countermeasures"
date = "2023-05-12"
draft = true
tags = ["security", "web-scraping", "automation"]
+++

Not every website wants to let the data to be scraped and not every app wants
to allow automation of user activity. If you work in scraping and automation 
at any capacity you certainly have dealt with sites that work just fine when 
accessed through normal browser throwing captchas or error pages at your bot. 
There are multiple security mechanisms that can cause this to happen. Today
we will do a broad review of automation countermeasures that can be implemented
at various levels.

IP level
========

The following techniques work at Internet Protocol level by allowing or blocking
traffic based on source IP address it is coming from.

Geofencing
----------

Geofencing (or geo-restricting/geo-blocking) is blocking/allowing requests
based on source geographic location (typically a country). This relies on GeoIP
data provided by vendors such as MaxMind or IP2Location to perform lookups.

As web scraper developer you can trivially bypass this using proxies.

Rate limiting
-------------

Web sites may impose upper bounds on on how many requests are allowed per some
timeframe from a single IP and start blocking incoming traffic if traffic
intensity exceeds that threshold. This is typically implemented with a leaky 
bucket algorithm. Suppose the site allows 60 requests per minute from a single
IP (i.e. one per seconds). It keeps a counter of how many requests it received,
decreasing it by one every second. If the counter exceeds the threshold value
(60) the site refuses further requests until it is below the threshold again.
This limits how aggressive we can be when scraping, but typically can be defeated
by spreading the traffic around a set of IP addresses by routing it through proxy
pool.

Filtering by AS/ISP
-------------------

This can be seen a variation of geofencing, but blocking can also be performed
based on source ISP or Autonomous System by performing Autonomous System
Lookups. For example, sites or antibot vendors may explicitly disallow traffic
that comes from data centers or major cloud vendors (AWS, Digital Ocean, Azure 
and so on).

Filtering by IP reputation
--------------------------

Proxies can be used to bypass the above ways to block traffic, but blocking
can also be performed based on IP address reputation data from threat 
intelligence vendors like IPQualityScore. So if your proxy provider is a 
sketchy one that is built on foundation of botnet it's proxies may be listed
in some blacklist, which makes the trust score bad. One must take care to use
a reputable proxy provider.

HTTPS level
===========

HTTP headers and cookies
------------------------

The very simplest way to know that traffic is automated is to check User-Agent
header in HTTP request. For example, curl puts something like `curl/8.0.1` in
there and some headless browsers actually let the server to know they are 
running in headless mode (unless configured otherwise). But that's trivial
to defeat. One baby step further is to not only check the User-Agent header,
but also other request headers to make sure like the ones in the browser in
aggregate. This is also easy to defeat. In fact, Chrome DevTools Network tab
has a handy feature of letting you conveniently copy a request as a curl command
or JavaScript (or even PowerShell!) snippet.

Cookies also matter when it comes to dealing with sites that are hostile to 
automation, as cookieless requests might get rejected. Depending on exact 
specifics of the site, getting the cookies might be as easy as just loading
the front page with `requests.Session` or it might involve more elaborate 
trickery if antibot vendor or captcha service is issuing them based on browser
assesment and user activity (more on that later).

HTTP/2 fingerprinting
---------------------

We covered the simple stuff, now lets get to something more complicated - 
HTTP/2 fingerprinting. HTTP/2 is quite complex binary protocol that leaves quite
a bit of wiggle room to implementor. The way it is implemented in curl is 
different than implementation in Google Chrome (altough both are RFC-compliant).
Traffic generated by each implementation can reliably betray what kind of 
implementation it is to the server, CDN or WAF. I have covered this in greated 
detail in [another post](/post/understanding-http2-fingerprinting/) earlier.

TLS fingerprinting
------------------

HTTP/2 is pretty much never used in plaintext form. After, you cannot let the
dirty people from NSA just watch the unencrypted traffic without any problems.
However, TLS protocol is also very complex. It involves a great deal
of technical details in the client traffic (protocol version, list of extensions,
proposed cryptographic protocols, etc.) that betray what kind of client
software it comes from. In fact, a single ClientHello message can be used to 
quite reliably predict the OS and browser that generated it. There's also 
[post](/post/understanding-tls-fingerprinting/) earlier in this blog to cover 
this topic in greater detail.

Application level
=================

JS environment checking
-----------------------

Modern web browsers are extremely complex software systems. They're not just
user-facing applications to load some pages from remote servers anymore. As of
2023, Chrome or Firefox contains an entire programmable environment for client
side software development that is primarily done in JS or variants thereof
(that end up being transpiled into JS anyway). Naturally this means that 
there's quite a bit of stuff in the JavaScript environment that sites can 
check for tell-tale signs of browser being controlled programmatically by 
automation software like like Selenium, Playwright or Puppetteer.

Some examples:

* `navigator.webdriver=true` generally betrays browser automation.
* Presence of `document.$cdc_asdjflasutopfhvcZLmcfl_` object betrays Selenium.
* On Chromium-based browsers, `window.chrome` object not being available means
the browser is running in headless mode.

Javascript challenges
---------------------

To make requests-based bots harder to implement, site may want to check if 
client is capable of running Javascript before giving the data. For example,
Cloudflare has an [Email Address Obfuscation](https://developers.cloudflare.com/support/more-dashboard-apps/cloudflare-scrape-shield/what-is-email-address-obfuscation/)
feature that relies on client-side JS code running in the browser to convert
the email address from encoded form in the HTML document your browser gets
from CDN to the normal form. Notice the difference between what you see in 
Elemnts tab of Chrome DevTools and in the original page source for the 
page of site using this feature.

[TODO: screenshots]

For this particular email obfuscation, there's a 
[Stack Overflow answer](https://stackoverflow.com/questions/48878687/using-python-to-scrape-information-from-a-cloudflare-site)
providing a Python snippet that reverts it back. However, if you are doing 
requests-based scraping and rely on regular expressions this sort of stuff can
already cause at least some difficulty. 

Furthermore, JS challenges can be more elaborate than this. Depending on 
how Cloudflare's antibot features are configured browser may be required to run 
some JS code with cryptographic proof of work stuff, provide the result of the 
computation to CF and only then get the cookie that allows it to properly access 
anything on the site.

Browser and device fingerprinting
---------------------------------

User level
==========

CAPTCHA
-------

Account-level throttling
------------------------

Account banning
---------------

Mouse activity monitoring
-------------------------

Making account creation difficult
---------------------------------

